								Projet SnakeRL


		I - Policy Gradients

Cette méthode vise à considérer la politique pi comme un réseau de neurones dont les paramètres sont modifiés au cours de l'entrainement en calculant des gradients.

	A - Théorie 

1) Formule
La formule de base est paramètres += learning_rate * Somme(grad(log(pi(a_t|s_t)))) * Somme(r_t)
Cette valeur étant moyenné sur un batch de taille N = 20.

2) Aspect intuitif
Intuitivement, on peut voir cette formule comme une modification des paramètres visant à maximiser que l'action a_t soit choisie dans l'état s_t et ce à chaque instant t. Cela peut paraître surprenant car on augmente alors toute les probabilités du moment que l'on en choisit une, mais il faut prendre en compte le coefficient Somme(r_t), qui fait qu'une action a_t sera plus favorisé dans un état s_t si la partie a été plus fructueuse.

3) Aspect mathématique
Mathématiquement, la formule vient du fait que notre paramètre idéal est l'argmax de l'espérance (pour une trajectoire respectant la distribution de paramètres idéal) du gain total Somme(r_t), cette espérance est noté J(theta).

Empiriquement, on a un estimateur sans biais de J avec la moyenne sur un batch de partie de Somme(r_t), mais c'est plus le gradient de J par rapport à theta que J en lui même qui nous intéresse.

L'espérance suivant pi de la somme des gains (où pi est la politique/distribution des trajectoires idéale) s'écrit mathématiquement comme l'intégrale de pi(theta, tau) * R(tau) où tau est la trajectoire (ensemble des a_t et s_t) et R(tau) la récompense globale.

4) Origine du logarithme dans la formule.
En calculant son gradient, le gradient passe dans l'intégrale, ne s'applique qu'à pi (car il est relatif à theta) et le gradient de pi donne le gradient de son logarithme multiplié par pi. Cela permet de revenir à une espérance puisqu'on a à nouveau pi en facteur dans l'intégrale. C'est de cela que vient la présence du logarithme.

5) Calcul empirique
On obtient plus précisément l'espérance suivant la distribution idéal de grad(log(pi(theta, tau))) * R(tau). Cette espérance est ensuite calculé empiriquement, avec pi(theta, tau) valant le produit des pi(a_t|s_t) et R(tau) la somme des r_t.




	B - Implémentation

Première implémentation : En implémentant avec la formule de base, avec un NN de trois couches dense de 64, 32 et 16 neurones, on effectue non sans mal notre premier test, peu concluant. L'agent semble bloquer dans des boucles horizontales ou verticales, en foncant dans une direction sans s'arrêter. Je pense que c'est du au fait que lorsqu'une action est prise elle est nécessairement favorisée par la suite. Cela déclenche un effet boule de neige ou l'agent fait plusieurs tours ce qui augmente d'autant plus son gradient. On parle de problème de high-variance du gradient, qui varie trop. 

La première solution, immédiate, est d'introduire un moyenneur de la récompense globale, c'est à dire de soustraire b à R(tau), où b est la moyenne des R(tau) sur le batch.

On peut aussi introduire le principe de causalité, qui dit qu'un choix de politique au temps t+x ne peut pas influencer la récompense au temps t, et que donc R(tau) devrait être remplacé par la somme des récompenses des temps suivants, car la récompense des temps antérieurs n'est de toute façon pas améliorable.
